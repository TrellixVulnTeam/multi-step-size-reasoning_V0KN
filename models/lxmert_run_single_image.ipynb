{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"lxmert_run_single_image.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1qVN3KYQ2q89zJsWpqFyMYsG_I9bcjJwc","authorship_tag":"ABX9TyPj2Cv5oAhRwimYgtLsZPey"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"DW6n9ohaaFPr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1596897394707,"user_tz":-120,"elapsed":4234,"user":{"displayName":"yulya sot","photoUrl":"https://lh6.googleusercontent.com/-ly9qpHtKZbM/AAAAAAAAAAI/AAAAAAAAAHs/3g5vNREGJNw/s64/photo.jpg","userId":"08271027061435931011"}},"outputId":"11fbdefb-4f8e-41fa-a2a0-3542a006bbc2"},"source":["!python --version"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Python 3.6.9\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"M0EPtn2zaPBL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":279},"executionInfo":{"status":"ok","timestamp":1596897400825,"user_tz":-120,"elapsed":10326,"user":{"displayName":"yulya sot","photoUrl":"https://lh6.googleusercontent.com/-ly9qpHtKZbM/AAAAAAAAAAI/AAAAAAAAAHs/3g5vNREGJNw/s64/photo.jpg","userId":"08271027061435931011"}},"outputId":"fcd8bbce-1844-478e-e778-4cafb1955242"},"source":["!git clone https://github.com/peteanderson80/bottom-up-attention/\n","!git clone https://github.com/airsplay/lxmert.git"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Cloning into 'bottom-up-attention'...\n","remote: Enumerating objects: 3, done.\u001b[K\n","remote: Counting objects: 100% (3/3), done.\u001b[K\n","remote: Compressing objects: 100% (3/3), done.\u001b[K\n","remote: Total 1468 (delta 0), reused 1 (delta 0), pack-reused 1465\u001b[K\n","Receiving objects: 100% (1468/1468), 13.76 MiB | 30.43 MiB/s, done.\n","Resolving deltas: 100% (648/648), done.\n","Cloning into 'lxmert'...\n","remote: Enumerating objects: 3, done.\u001b[K\n","remote: Counting objects: 100% (3/3), done.\u001b[K\n","remote: Compressing objects: 100% (3/3), done.\u001b[K\n","remote: Total 340 (delta 0), reused 1 (delta 0), pack-reused 337\u001b[K\n","Receiving objects: 100% (340/340), 259.36 KiB | 10.81 MiB/s, done.\n","Resolving deltas: 100% (186/186), done.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vAmydNZGacIH","colab_type":"text"},"source":["Download pretrained LXMERT"]},{"cell_type":"code","metadata":{"id":"h9O5TMNbaV_y","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":260},"executionInfo":{"status":"ok","timestamp":1594648645554,"user_tz":-120,"elapsed":88804,"user":{"displayName":"yulya sot","photoUrl":"https://lh6.googleusercontent.com/-ly9qpHtKZbM/AAAAAAAAAAI/AAAAAAAAAHs/3g5vNREGJNw/s64/photo.jpg","userId":"08271027061435931011"}},"outputId":"d1315453-008f-4d3d-b579-b5ccab787603"},"source":["!cd lxmert/ && wget --no-check-certificate https://nlp1.cs.unc.edu/data/model_LXRT.pth -P snap/pretrained"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2020-07-13 13:56:25--  https://nlp1.cs.unc.edu/data/model_LXRT.pth\n","Resolving nlp1.cs.unc.edu (nlp1.cs.unc.edu)... 152.2.142.178\n","Connecting to nlp1.cs.unc.edu (nlp1.cs.unc.edu)|152.2.142.178|:443... connected.\n","WARNING: no certificate subject alternative name matches\n","\trequested host name ‘nlp1.cs.unc.edu’.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 912336661 (870M)\n","Saving to: ‘snap/pretrained/model_LXRT.pth’\n","\n","model_LXRT.pth      100%[===================>] 870.07M  16.0MB/s    in 56s     \n","\n","2020-07-13 13:57:23 (15.4 MB/s) - ‘snap/pretrained/model_LXRT.pth’ saved [912336661/912336661]\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IaVDsyC_akSo","colab_type":"text"},"source":["Download extracted visual features from Drive: single image"]},{"cell_type":"code","metadata":{"id":"CheZPvJmagMe","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":92},"executionInfo":{"status":"ok","timestamp":1596897423804,"user_tz":-120,"elapsed":33009,"user":{"displayName":"yulya sot","photoUrl":"https://lh6.googleusercontent.com/-ly9qpHtKZbM/AAAAAAAAAAI/AAAAAAAAAHs/3g5vNREGJNw/s64/photo.jpg","userId":"08271027061435931011"}},"outputId":"83a7ee35-06de-4b92-c576-81a478f44eb1"},"source":["!cd /content/lxmert/data/nlvr2_imgfeat/ && gdown --id 1-HT36QtTHGO27G-xYtRnQNr2U8TQYudZ"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading...\n","From: https://drive.google.com/uc?id=1-HT36QtTHGO27G-xYtRnQNr2U8TQYudZ\n","To: /content/lxmert/data/nlvr2_imgfeat/test_obj36.tsv\n","790MB [00:10, 78.2MB/s]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"i9LsOmpUak1S","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":92},"executionInfo":{"status":"ok","timestamp":1596897431556,"user_tz":-120,"elapsed":40269,"user":{"displayName":"yulya sot","photoUrl":"https://lh6.googleusercontent.com/-ly9qpHtKZbM/AAAAAAAAAAI/AAAAAAAAAHs/3g5vNREGJNw/s64/photo.jpg","userId":"08271027061435931011"}},"outputId":"2a5146b2-8a5c-4f35-ce71-fc12ec68c590"},"source":["!cd /content/lxmert/data/nlvr2_imgfeat/ && gdown \"https://drive.google.com/uc?id=1-HcH61XrFpHm2jffR1cjaB8tjI5dY64R\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading...\n","From: https://drive.google.com/uc?id=1-HcH61XrFpHm2jffR1cjaB8tjI5dY64R\n","To: /content/lxmert/data/nlvr2_imgfeat/valid_obj36.tsv\n","790MB [00:05, 138MB/s]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hk8wUdBJaqiP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":92},"executionInfo":{"status":"ok","timestamp":1596897509557,"user_tz":-120,"elapsed":117874,"user":{"displayName":"yulya sot","photoUrl":"https://lh6.googleusercontent.com/-ly9qpHtKZbM/AAAAAAAAAAI/AAAAAAAAAHs/3g5vNREGJNw/s64/photo.jpg","userId":"08271027061435931011"}},"outputId":"e6d7deff-ae1c-4d05-b979-ac8abe6ffa4b"},"source":["!cd /content/lxmert/data/nlvr2_imgfeat/ && gdown \"https://drive.google.com/uc?id=1-GK6Dv1aPYxjlBSbe_QdJAXe1mhgPgJ1\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading...\n","From: https://drive.google.com/uc?id=1-GK6Dv1aPYxjlBSbe_QdJAXe1mhgPgJ1\n","To: /content/lxmert/data/nlvr2_imgfeat/train_obj36.tsv\n","6.32GB [01:15, 83.8MB/s]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uSHr82AKbC93","colab_type":"code","colab":{}},"source":["!cp \"/content/drive/My Drive/out_single/test_obj36.tsv\" \"/content/lxmert/data/nlvr2_imgfeat/\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_fnIROl2bU_W","colab_type":"code","colab":{}},"source":["!cp \"/content/drive/My Drive/out_single/train_obj36.tsv\" \"/content/lxmert/data/nlvr2_imgfeat/\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2Ot-ttyVbceE","colab_type":"code","colab":{}},"source":["!cp \"/content/drive/My Drive/out_single/valid_obj36.tsv\" \"/content/lxmert/data/nlvr2_imgfeat/\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7HqdgCmZatuW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1594650478910,"user_tz":-120,"elapsed":1580125,"user":{"displayName":"yulya sot","photoUrl":"https://lh6.googleusercontent.com/-ly9qpHtKZbM/AAAAAAAAAAI/AAAAAAAAAHs/3g5vNREGJNw/s64/photo.jpg","userId":"08271027061435931011"}},"outputId":"6660e149-0e0a-4b38-8851-5b2c5afe8fc9"},"source":["!cd lxmert/ && bash run/nlvr2_finetune.bash 0 nlvr2_lxr955 --epochs 150 --lr 1e-5"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Load 16000 data from split(s) train.\n","Start to load Faster-RCNN detected objects from data/nlvr2_imgfeat/train_obj36.tsv\n","Loaded 16000 images in file data/nlvr2_imgfeat/train_obj36.tsv in 78 seconds.\n","Use 16000 data in torch dataset\n","\n","Load 2000 data from split(s) valid.\n","Start to load Faster-RCNN detected objects from data/nlvr2_imgfeat/valid_obj36.tsv\n","Loaded 2000 images in file data/nlvr2_imgfeat/valid_obj36.tsv in 10 seconds.\n","Use 2000 data in torch dataset\n","\n","100% 231508/231508 [00:00<00:00, 321338.62B/s]\n","100% 407873900/407873900 [00:31<00:00, 12768600.87B/s]\n","LXRT encoder with 9 l_layers, 5 x_layers, and 5 r_layers.\n","Load LXMERT pre-trained model from snap/pretrained/model\n","\n","Weights in loaded but not in model:\n","answer_head.logit_fc.0.bias\n","answer_head.logit_fc.0.weight\n","answer_head.logit_fc.2.bias\n","answer_head.logit_fc.2.weight\n","answer_head.logit_fc.3.bias\n","answer_head.logit_fc.3.weight\n","cls.predictions.bias\n","cls.predictions.decoder.weight\n","cls.predictions.transform.LayerNorm.bias\n","cls.predictions.transform.LayerNorm.weight\n","cls.predictions.transform.dense.bias\n","cls.predictions.transform.dense.weight\n","cls.seq_relationship.bias\n","cls.seq_relationship.weight\n","obj_predict_head.decoder_dict.attr.bias\n","obj_predict_head.decoder_dict.attr.weight\n","obj_predict_head.decoder_dict.feat.bias\n","obj_predict_head.decoder_dict.feat.weight\n","obj_predict_head.decoder_dict.obj.bias\n","obj_predict_head.decoder_dict.obj.weight\n","obj_predict_head.transform.LayerNorm.bias\n","obj_predict_head.transform.LayerNorm.weight\n","obj_predict_head.transform.dense.bias\n","obj_predict_head.transform.dense.weight\n","\n","Weights in model but not in loaded:\n","\n","Total Iters: 75000\n","Splits in Train data: ['train']\n","Splits in Valid data: ['valid']\n","  0% 0/500 [00:00<?, ?it/s]/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha)\n","100% 500/500 [02:29<00:00,  3.34it/s]\n","\n","Loss: tensor(0.7227, device='cuda:0', grad_fn=<NllLossBackward>)\n","\n","Epoch 0: Train 50.70\n","Epoch 0: Valid 50.15\n","Epoch 0: Best 50.15\n","100% 500/500 [02:28<00:00,  3.36it/s]\n","\n","Loss: tensor(0.2894, device='cuda:0', grad_fn=<NllLossBackward>)\n","\n","Epoch 1: Train 67.67\n","Epoch 1: Valid 87.35\n","Epoch 1: Best 87.35\n","100% 500/500 [02:29<00:00,  3.35it/s]\n","\n","Loss: tensor(0.1119, device='cuda:0', grad_fn=<NllLossBackward>)\n","\n","Epoch 2: Train 88.52\n","Epoch 2: Valid 91.30\n","Epoch 2: Best 91.30\n","100% 500/500 [02:29<00:00,  3.34it/s]\n","\n","Loss: tensor(0.1620, device='cuda:0', grad_fn=<NllLossBackward>)\n","\n","Epoch 3: Train 91.45\n","Epoch 3: Valid 93.00\n","Epoch 3: Best 93.00\n","100% 500/500 [02:29<00:00,  3.35it/s]\n","\n","Loss: tensor(0.1126, device='cuda:0', grad_fn=<NllLossBackward>)\n","\n","Epoch 4: Train 92.77\n","Epoch 4: Valid 93.15\n","Epoch 4: Best 93.15\n","100% 500/500 [02:28<00:00,  3.36it/s]\n","\n","Loss: tensor(0.1350, device='cuda:0', grad_fn=<NllLossBackward>)\n","\n","Epoch 5: Train 93.66\n","Epoch 5: Valid 93.90\n","Epoch 5: Best 93.90\n","100% 500/500 [02:28<00:00,  3.36it/s]\n","\n","Loss: tensor(0.2516, device='cuda:0', grad_fn=<NllLossBackward>)\n","\n","Epoch 6: Train 94.42\n","Epoch 6: Valid 94.10\n","Epoch 6: Best 94.10\n","100% 500/500 [02:28<00:00,  3.37it/s]\n","\n","Loss: tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward>)\n","\n","Epoch 7: Train 95.16\n","Epoch 7: Valid 94.30\n","Epoch 7: Best 94.30\n","100% 500/500 [02:28<00:00,  3.37it/s]\n","\n","Loss: tensor(0.0354, device='cuda:0', grad_fn=<NllLossBackward>)\n","\n","Epoch 8: Train 95.45\n","Epoch 8: Valid 94.25\n","Epoch 8: Best 94.30\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eTZ8ZDg3n0N1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":372},"executionInfo":{"status":"ok","timestamp":1592571075180,"user_tz":-120,"elapsed":72325,"user":{"displayName":"yulya sot","photoUrl":"https://lh6.googleusercontent.com/-ly9qpHtKZbM/AAAAAAAAAAI/AAAAAAAAAHs/3g5vNREGJNw/s64/photo.jpg","userId":"08271027061435931011"}},"outputId":"293fc349-aa41-449a-e7c8-9a59fc0a9550"},"source":["!cd lxmert/ && bash run/nlvr2_test.bash 0 nlvr2_lxr955_results --load snap/nlvr2/nlvr2_lxr955/BEST --test test --batchSize 1024"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Load 16000 data from split(s) train.\n","Start to load Faster-RCNN detected objects from data/nlvr2_imgfeat/train_obj36.tsv\n","Loaded 512 images in file data/nlvr2_imgfeat/train_obj36.tsv in 2 seconds.\n","Use 512 data in torch dataset\n","\n","Load 2000 data from split(s) valid.\n","Start to load Faster-RCNN detected objects from data/nlvr2_imgfeat/valid_obj36.tsv\n","Loaded 512 images in file data/nlvr2_imgfeat/valid_obj36.tsv in 5 seconds.\n","Use 512 data in torch dataset\n","\n","LXRT encoder with 9 l_layers, 5 x_layers, and 5 r_layers.\n","Total Iters: 0\n","Load model from snap/nlvr2/nlvr2_lxr955/BEST\n","Load 2000 data from split(s) test.\n","Start to load Faster-RCNN detected objects from data/nlvr2_imgfeat/test_obj36.tsv\n","Loaded 2000 images in file data/nlvr2_imgfeat/test_obj36.tsv in 21 seconds.\n","Use 2000 data in torch dataset\n","\n","0.935\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Lg55sn3TcxKl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1592572987477,"user_tz":-120,"elapsed":1984602,"user":{"displayName":"yulya sot","photoUrl":"https://lh6.googleusercontent.com/-ly9qpHtKZbM/AAAAAAAAAAI/AAAAAAAAAHs/3g5vNREGJNw/s64/photo.jpg","userId":"08271027061435931011"}},"outputId":"afa004f6-3565-4565-8ccf-bdbc4302cfc0"},"source":["!cd lxmert/ && bash run/nlvr2_finetune.bash 0 nlvr2_lxr955 --epochs 150 --lr 1e-5 --seed 12321\n","!cd lxmert/ && bash run/nlvr2_test.bash 0 nlvr2_lxr955_results --load snap/nlvr2/nlvr2_lxr955/BEST --test test --batchSize 1024"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Load 16000 data from split(s) train.\n","Start to load Faster-RCNN detected objects from data/nlvr2_imgfeat/train_obj36.tsv\n","Loaded 16000 images in file data/nlvr2_imgfeat/train_obj36.tsv in 99 seconds.\n","Use 16000 data in torch dataset\n","\n","Load 2000 data from split(s) valid.\n","Start to load Faster-RCNN detected objects from data/nlvr2_imgfeat/valid_obj36.tsv\n","Loaded 2000 images in file data/nlvr2_imgfeat/valid_obj36.tsv in 18 seconds.\n","Use 2000 data in torch dataset\n","\n","LXRT encoder with 9 l_layers, 5 x_layers, and 5 r_layers.\n","Load LXMERT pre-trained model from snap/pretrained/model\n","\n","Weights in loaded but not in model:\n","answer_head.logit_fc.0.bias\n","answer_head.logit_fc.0.weight\n","answer_head.logit_fc.2.bias\n","answer_head.logit_fc.2.weight\n","answer_head.logit_fc.3.bias\n","answer_head.logit_fc.3.weight\n","cls.predictions.bias\n","cls.predictions.decoder.weight\n","cls.predictions.transform.LayerNorm.bias\n","cls.predictions.transform.LayerNorm.weight\n","cls.predictions.transform.dense.bias\n","cls.predictions.transform.dense.weight\n","cls.seq_relationship.bias\n","cls.seq_relationship.weight\n","obj_predict_head.decoder_dict.attr.bias\n","obj_predict_head.decoder_dict.attr.weight\n","obj_predict_head.decoder_dict.feat.bias\n","obj_predict_head.decoder_dict.feat.weight\n","obj_predict_head.decoder_dict.obj.bias\n","obj_predict_head.decoder_dict.obj.weight\n","obj_predict_head.transform.LayerNorm.bias\n","obj_predict_head.transform.LayerNorm.weight\n","obj_predict_head.transform.dense.bias\n","obj_predict_head.transform.dense.weight\n","\n","Weights in model but not in loaded:\n","\n","Total Iters: 75000\n","Splits in Train data: ['train']\n","Splits in Valid data: ['valid']\n","  0% 0/500 [00:00<?, ?it/s]/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha)\n","100% 500/500 [03:01<00:00,  2.75it/s]\n","\n","Loss: tensor(0.7213, device='cuda:0', grad_fn=<NllLossBackward>)\n","\n","Epoch 0: Train 50.98\n","Epoch 0: Valid 56.40\n","Epoch 0: Best 56.40\n","100% 500/500 [03:01<00:00,  2.76it/s]\n","\n","Loss: tensor(0.1644, device='cuda:0', grad_fn=<NllLossBackward>)\n","\n","Epoch 1: Train 73.78\n","Epoch 1: Valid 87.75\n","Epoch 1: Best 87.75\n","100% 500/500 [03:01<00:00,  2.76it/s]\n","\n","Loss: tensor(0.4505, device='cuda:0', grad_fn=<NllLossBackward>)\n","\n","Epoch 2: Train 88.68\n","Epoch 2: Valid 90.40\n","Epoch 2: Best 90.40\n","100% 500/500 [03:00<00:00,  2.76it/s]\n","\n","Loss: tensor(0.1265, device='cuda:0', grad_fn=<NllLossBackward>)\n","\n","Epoch 3: Train 91.70\n","Epoch 3: Valid 92.75\n","Epoch 3: Best 92.75\n","100% 500/500 [02:59<00:00,  2.79it/s]\n","\n","Loss: tensor(0.2457, device='cuda:0', grad_fn=<NllLossBackward>)\n","\n","Epoch 4: Train 92.86\n","Epoch 4: Valid 93.25\n","Epoch 4: Best 93.25\n","100% 500/500 [03:00<00:00,  2.77it/s]\n","\n","Loss: tensor(0.3102, device='cuda:0', grad_fn=<NllLossBackward>)\n","\n","Epoch 5: Train 93.86\n","Epoch 5: Valid 93.20\n","Epoch 5: Best 93.25\n","100% 500/500 [03:00<00:00,  2.78it/s]\n","\n","Loss: tensor(0.1154, device='cuda:0', grad_fn=<NllLossBackward>)\n","\n","Epoch 6: Train 94.44\n","Epoch 6: Valid 93.80\n","Epoch 6: Best 93.80\n","100% 500/500 [02:59<00:00,  2.79it/s]\n","\n","Loss: tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward>)\n","\n","Epoch 7: Train 94.91\n","Epoch 7: Valid 94.00\n","Epoch 7: Best 94.00\n","100% 500/500 [02:59<00:00,  2.78it/s]\n","\n","Loss: tensor(0.0889, device='cuda:0', grad_fn=<NllLossBackward>)\n","\n","Epoch 8: Train 95.28\n","Epoch 8: Valid 94.15\n","Epoch 8: Best 94.15\n","Load 16000 data from split(s) train.\n","Start to load Faster-RCNN detected objects from data/nlvr2_imgfeat/train_obj36.tsv\n","Loaded 512 images in file data/nlvr2_imgfeat/train_obj36.tsv in 2 seconds.\n","Use 512 data in torch dataset\n","\n","Load 2000 data from split(s) valid.\n","Start to load Faster-RCNN detected objects from data/nlvr2_imgfeat/valid_obj36.tsv\n","Loaded 512 images in file data/nlvr2_imgfeat/valid_obj36.tsv in 2 seconds.\n","Use 512 data in torch dataset\n","\n","LXRT encoder with 9 l_layers, 5 x_layers, and 5 r_layers.\n","Total Iters: 0\n","Load model from snap/nlvr2/nlvr2_lxr955/BEST\n","Load 2000 data from split(s) test.\n","Start to load Faster-RCNN detected objects from data/nlvr2_imgfeat/test_obj36.tsv\n","Loaded 2000 images in file data/nlvr2_imgfeat/test_obj36.tsv in 21 seconds.\n","Use 2000 data in torch dataset\n","\n","0.9395\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZVnnpNgzk4O5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1592574869271,"user_tz":-120,"elapsed":3854223,"user":{"displayName":"yulya sot","photoUrl":"https://lh6.googleusercontent.com/-ly9qpHtKZbM/AAAAAAAAAAI/AAAAAAAAAHs/3g5vNREGJNw/s64/photo.jpg","userId":"08271027061435931011"}},"outputId":"85ab40eb-4fa0-4f08-bdd2-3c9232cf64ff"},"source":["!cd lxmert/ && bash run/nlvr2_finetune.bash 0 nlvr2_lxr955 --epochs 150 --lr 1e-5 --seed 2\n","!cd lxmert/ && bash run/nlvr2_test.bash 0 nlvr2_lxr955_results --load snap/nlvr2/nlvr2_lxr955/BEST --test test --batchSize 1024"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Load 16000 data from split(s) train.\n","Start to load Faster-RCNN detected objects from data/nlvr2_imgfeat/train_obj36.tsv\n","Loaded 16000 images in file data/nlvr2_imgfeat/train_obj36.tsv in 102 seconds.\n","Use 16000 data in torch dataset\n","\n","Load 2000 data from split(s) valid.\n","Start to load Faster-RCNN detected objects from data/nlvr2_imgfeat/valid_obj36.tsv\n","Loaded 2000 images in file data/nlvr2_imgfeat/valid_obj36.tsv in 18 seconds.\n","Use 2000 data in torch dataset\n","\n","LXRT encoder with 9 l_layers, 5 x_layers, and 5 r_layers.\n","Load LXMERT pre-trained model from snap/pretrained/model\n","\n","Weights in loaded but not in model:\n","answer_head.logit_fc.0.bias\n","answer_head.logit_fc.0.weight\n","answer_head.logit_fc.2.bias\n","answer_head.logit_fc.2.weight\n","answer_head.logit_fc.3.bias\n","answer_head.logit_fc.3.weight\n","cls.predictions.bias\n","cls.predictions.decoder.weight\n","cls.predictions.transform.LayerNorm.bias\n","cls.predictions.transform.LayerNorm.weight\n","cls.predictions.transform.dense.bias\n","cls.predictions.transform.dense.weight\n","cls.seq_relationship.bias\n","cls.seq_relationship.weight\n","obj_predict_head.decoder_dict.attr.bias\n","obj_predict_head.decoder_dict.attr.weight\n","obj_predict_head.decoder_dict.feat.bias\n","obj_predict_head.decoder_dict.feat.weight\n","obj_predict_head.decoder_dict.obj.bias\n","obj_predict_head.decoder_dict.obj.weight\n","obj_predict_head.transform.LayerNorm.bias\n","obj_predict_head.transform.LayerNorm.weight\n","obj_predict_head.transform.dense.bias\n","obj_predict_head.transform.dense.weight\n","\n","Weights in model but not in loaded:\n","\n","Total Iters: 75000\n","Splits in Train data: ['train']\n","Splits in Valid data: ['valid']\n","  0% 0/500 [00:00<?, ?it/s]/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha)\n","100% 500/500 [02:59<00:00,  2.79it/s]\n","\n","Loss: tensor(0.6684, device='cuda:0', grad_fn=<NllLossBackward>)\n","\n","Epoch 0: Train 50.39\n","Epoch 0: Valid 50.05\n","Epoch 0: Best 50.05\n","100% 500/500 [02:59<00:00,  2.79it/s]\n","\n","Loss: tensor(0.3016, device='cuda:0', grad_fn=<NllLossBackward>)\n","\n","Epoch 1: Train 65.49\n","Epoch 1: Valid 86.75\n","Epoch 1: Best 86.75\n","100% 500/500 [02:58<00:00,  2.80it/s]\n","\n","Loss: tensor(0.2349, device='cuda:0', grad_fn=<NllLossBackward>)\n","\n","Epoch 2: Train 88.82\n","Epoch 2: Valid 90.50\n","Epoch 2: Best 90.50\n","100% 500/500 [02:59<00:00,  2.79it/s]\n","\n","Loss: tensor(0.2683, device='cuda:0', grad_fn=<NllLossBackward>)\n","\n","Epoch 3: Train 91.76\n","Epoch 3: Valid 93.30\n","Epoch 3: Best 93.30\n","100% 500/500 [02:58<00:00,  2.79it/s]\n","\n","Loss: tensor(0.1168, device='cuda:0', grad_fn=<NllLossBackward>)\n","\n","Epoch 4: Train 92.93\n","Epoch 4: Valid 93.20\n","Epoch 4: Best 93.30\n","100% 500/500 [03:00<00:00,  2.77it/s]\n","\n","Loss: tensor(0.1042, device='cuda:0', grad_fn=<NllLossBackward>)\n","\n","Epoch 5: Train 94.14\n","Epoch 5: Valid 94.00\n","Epoch 5: Best 94.00\n","100% 500/500 [02:58<00:00,  2.80it/s]\n","\n","Loss: tensor(0.2587, device='cuda:0', grad_fn=<NllLossBackward>)\n","\n","Epoch 6: Train 94.47\n","Epoch 6: Valid 93.60\n","Epoch 6: Best 94.00\n","100% 500/500 [02:58<00:00,  2.80it/s]\n","\n","Loss: tensor(0.1010, device='cuda:0', grad_fn=<NllLossBackward>)\n","\n","Epoch 7: Train 94.99\n","Epoch 7: Valid 94.50\n","Epoch 7: Best 94.50\n","100% 500/500 [02:59<00:00,  2.79it/s]\n","\n","Loss: tensor(0.0458, device='cuda:0', grad_fn=<NllLossBackward>)\n","\n","Epoch 8: Train 95.30\n","Epoch 8: Valid 93.30\n","Epoch 8: Best 94.50\n","Load 16000 data from split(s) train.\n","Start to load Faster-RCNN detected objects from data/nlvr2_imgfeat/train_obj36.tsv\n","Loaded 512 images in file data/nlvr2_imgfeat/train_obj36.tsv in 2 seconds.\n","Use 512 data in torch dataset\n","\n","Load 2000 data from split(s) valid.\n","Start to load Faster-RCNN detected objects from data/nlvr2_imgfeat/valid_obj36.tsv\n","Loaded 512 images in file data/nlvr2_imgfeat/valid_obj36.tsv in 2 seconds.\n","Use 512 data in torch dataset\n","\n","LXRT encoder with 9 l_layers, 5 x_layers, and 5 r_layers.\n","Total Iters: 0\n","Load model from snap/nlvr2/nlvr2_lxr955/BEST\n","Load 2000 data from split(s) test.\n","Start to load Faster-RCNN detected objects from data/nlvr2_imgfeat/test_obj36.tsv\n","Loaded 2000 images in file data/nlvr2_imgfeat/test_obj36.tsv in 9 seconds.\n","Use 2000 data in torch dataset\n","\n","0.933\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Tvp4aH9VlFc-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1592579569660,"user_tz":-120,"elapsed":877478,"user":{"displayName":"yulya sot","photoUrl":"https://lh6.googleusercontent.com/-ly9qpHtKZbM/AAAAAAAAAAI/AAAAAAAAAHs/3g5vNREGJNw/s64/photo.jpg","userId":"08271027061435931011"}},"outputId":"0817c02b-19b5-46fc-b4eb-f549f8381ef0"},"source":["!cd lxmert/ && bash run/nlvr2_finetune.bash 0 nlvr2_lxr955 --epochs 150 --lr 1e-5 --seed 1234\n","!cd lxmert/ && bash run/nlvr2_test.bash 0 nlvr2_lxr955_results --load snap/nlvr2/nlvr2_lxr955/BEST --test test --batchSize 1024"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Load 16000 data from split(s) train.\n","Start to load Faster-RCNN detected objects from data/nlvr2_imgfeat/train_obj36.tsv\n","Loaded 16000 images in file data/nlvr2_imgfeat/train_obj36.tsv in 81 seconds.\n","Use 16000 data in torch dataset\n","\n","Load 2000 data from split(s) valid.\n","Start to load Faster-RCNN detected objects from data/nlvr2_imgfeat/valid_obj36.tsv\n","Loaded 2000 images in file data/nlvr2_imgfeat/valid_obj36.tsv in 10 seconds.\n","Use 2000 data in torch dataset\n","\n","LXRT encoder with 9 l_layers, 5 x_layers, and 5 r_layers.\n","Load LXMERT pre-trained model from snap/pretrained/model\n","\n","Weights in loaded but not in model:\n","answer_head.logit_fc.0.bias\n","answer_head.logit_fc.0.weight\n","answer_head.logit_fc.2.bias\n","answer_head.logit_fc.2.weight\n","answer_head.logit_fc.3.bias\n","answer_head.logit_fc.3.weight\n","cls.predictions.bias\n","cls.predictions.decoder.weight\n","cls.predictions.transform.LayerNorm.bias\n","cls.predictions.transform.LayerNorm.weight\n","cls.predictions.transform.dense.bias\n","cls.predictions.transform.dense.weight\n","cls.seq_relationship.bias\n","cls.seq_relationship.weight\n","obj_predict_head.decoder_dict.attr.bias\n","obj_predict_head.decoder_dict.attr.weight\n","obj_predict_head.decoder_dict.feat.bias\n","obj_predict_head.decoder_dict.feat.weight\n","obj_predict_head.decoder_dict.obj.bias\n","obj_predict_head.decoder_dict.obj.weight\n","obj_predict_head.transform.LayerNorm.bias\n","obj_predict_head.transform.LayerNorm.weight\n","obj_predict_head.transform.dense.bias\n","obj_predict_head.transform.dense.weight\n","\n","Weights in model but not in loaded:\n","\n","Total Iters: 75000\n","Splits in Train data: ['train']\n","Splits in Valid data: ['valid']\n","  0% 0/500 [00:00<?, ?it/s]/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha)\n","100% 500/500 [02:58<00:00,  2.80it/s]\n","\n","Loss: tensor(0.7033, device='cuda:0', grad_fn=<NllLossBackward>)\n","\n","Epoch 0: Train 50.19\n","Epoch 0: Valid 55.90\n","Epoch 0: Best 55.90\n","100% 500/500 [02:58<00:00,  2.80it/s]\n","\n","Loss: tensor(0.2620, device='cuda:0', grad_fn=<NllLossBackward>)\n","\n","Epoch 1: Train 71.88\n","Epoch 1: Valid 86.65\n","Epoch 1: Best 86.65\n","100% 500/500 [02:58<00:00,  2.80it/s]\n","\n","Loss: tensor(0.2186, device='cuda:0', grad_fn=<NllLossBackward>)\n","\n","Epoch 2: Train 88.52\n","Epoch 2: Valid 91.50\n","Epoch 2: Best 91.50\n","100% 500/500 [02:59<00:00,  2.78it/s]\n","\n","Loss: tensor(0.1149, device='cuda:0', grad_fn=<NllLossBackward>)\n","\n","Epoch 3: Train 91.45\n","Epoch 3: Valid 93.05\n","Epoch 3: Best 93.05\n","100% 500/500 [02:58<00:00,  2.80it/s]\n","\n","Loss: tensor(0.2927, device='cuda:0', grad_fn=<NllLossBackward>)\n","\n","Epoch 4: Train 92.59\n","Epoch 4: Valid 93.55\n","Epoch 4: Best 93.55\n","100% 500/500 [02:58<00:00,  2.80it/s]\n","\n","Loss: tensor(0.0795, device='cuda:0', grad_fn=<NllLossBackward>)\n","\n","Epoch 5: Train 93.94\n","Epoch 5: Valid 94.05\n","Epoch 5: Best 94.05\n","100% 500/500 [02:58<00:00,  2.80it/s]\n","\n","Loss: tensor(0.2544, device='cuda:0', grad_fn=<NllLossBackward>)\n","\n","Epoch 6: Train 94.16\n","Epoch 6: Valid 94.45\n","Epoch 6: Best 94.45\n","100% 500/500 [02:56<00:00,  2.83it/s]\n","\n","Loss: tensor(0.0229, device='cuda:0', grad_fn=<NllLossBackward>)\n","\n","Epoch 7: Train 94.88\n","Epoch 7: Valid 93.70\n","Epoch 7: Best 94.45\n","100% 500/500 [02:56<00:00,  2.83it/s]\n","\n","Loss: tensor(0.1003, device='cuda:0', grad_fn=<NllLossBackward>)\n","\n","Epoch 8: Train 95.45\n","Epoch 8: Valid 94.30\n","Epoch 8: Best 94.45\n","Load 16000 data from split(s) train.\n","Start to load Faster-RCNN detected objects from data/nlvr2_imgfeat/train_obj36.tsv\n","Loaded 512 images in file data/nlvr2_imgfeat/train_obj36.tsv in 2 seconds.\n","Use 512 data in torch dataset\n","\n","Load 2000 data from split(s) valid.\n","Start to load Faster-RCNN detected objects from data/nlvr2_imgfeat/valid_obj36.tsv\n","Loaded 512 images in file data/nlvr2_imgfeat/valid_obj36.tsv in 2 seconds.\n","Use 512 data in torch dataset\n","\n","LXRT encoder with 9 l_layers, 5 x_layers, and 5 r_layers.\n","Total Iters: 0\n","Load model from snap/nlvr2/nlvr2_lxr955/BEST\n","Load 2000 data from split(s) test.\n","Start to load Faster-RCNN detected objects from data/nlvr2_imgfeat/test_obj36.tsv\n","Loaded 2000 images in file data/nlvr2_imgfeat/test_obj36.tsv in 9 seconds.\n","Use 2000 data in torch dataset\n","\n","0.9285\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"E17htCLPlI3M","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1592581400120,"user_tz":-120,"elapsed":1830481,"user":{"displayName":"yulya sot","photoUrl":"https://lh6.googleusercontent.com/-ly9qpHtKZbM/AAAAAAAAAAI/AAAAAAAAAHs/3g5vNREGJNw/s64/photo.jpg","userId":"08271027061435931011"}},"outputId":"5f7d18b4-a49a-4a8f-f207-d9ce2727c8c9"},"source":["!cd lxmert/ && bash run/nlvr2_finetune.bash 0 nlvr2_lxr955 --epochs 150 --lr 1e-5 --seed 42\n","!cd lxmert/ && bash run/nlvr2_test.bash 0 nlvr2_lxr955_results --load snap/nlvr2/nlvr2_lxr955/BEST --test test --batchSize 1024"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Load 16000 data from split(s) train.\n","Start to load Faster-RCNN detected objects from data/nlvr2_imgfeat/train_obj36.tsv\n","Loaded 16000 images in file data/nlvr2_imgfeat/train_obj36.tsv in 82 seconds.\n","Use 16000 data in torch dataset\n","\n","Load 2000 data from split(s) valid.\n","Start to load Faster-RCNN detected objects from data/nlvr2_imgfeat/valid_obj36.tsv\n","Loaded 2000 images in file data/nlvr2_imgfeat/valid_obj36.tsv in 10 seconds.\n","Use 2000 data in torch dataset\n","\n","LXRT encoder with 9 l_layers, 5 x_layers, and 5 r_layers.\n","Load LXMERT pre-trained model from snap/pretrained/model\n","\n","Weights in loaded but not in model:\n","answer_head.logit_fc.0.bias\n","answer_head.logit_fc.0.weight\n","answer_head.logit_fc.2.bias\n","answer_head.logit_fc.2.weight\n","answer_head.logit_fc.3.bias\n","answer_head.logit_fc.3.weight\n","cls.predictions.bias\n","cls.predictions.decoder.weight\n","cls.predictions.transform.LayerNorm.bias\n","cls.predictions.transform.LayerNorm.weight\n","cls.predictions.transform.dense.bias\n","cls.predictions.transform.dense.weight\n","cls.seq_relationship.bias\n","cls.seq_relationship.weight\n","obj_predict_head.decoder_dict.attr.bias\n","obj_predict_head.decoder_dict.attr.weight\n","obj_predict_head.decoder_dict.feat.bias\n","obj_predict_head.decoder_dict.feat.weight\n","obj_predict_head.decoder_dict.obj.bias\n","obj_predict_head.decoder_dict.obj.weight\n","obj_predict_head.transform.LayerNorm.bias\n","obj_predict_head.transform.LayerNorm.weight\n","obj_predict_head.transform.dense.bias\n","obj_predict_head.transform.dense.weight\n","\n","Weights in model but not in loaded:\n","\n","Total Iters: 75000\n","Splits in Train data: ['train']\n","Splits in Valid data: ['valid']\n","  0% 0/500 [00:00<?, ?it/s]/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha)\n","100% 500/500 [03:00<00:00,  2.77it/s]\n","\n","Loss: tensor(0.6681, device='cuda:0', grad_fn=<NllLossBackward>)\n","\n","Epoch 0: Train 49.43\n","Epoch 0: Valid 51.65\n","Epoch 0: Best 51.65\n","100% 500/500 [02:58<00:00,  2.80it/s]\n","\n","Loss: tensor(0.6864, device='cuda:0', grad_fn=<NllLossBackward>)\n","\n","Epoch 1: Train 50.82\n","Epoch 1: Valid 51.55\n","Epoch 1: Best 51.65\n","100% 500/500 [03:00<00:00,  2.77it/s]\n","\n","Loss: tensor(0.2642, device='cuda:0', grad_fn=<NllLossBackward>)\n","\n","Epoch 2: Train 73.21\n","Epoch 2: Valid 90.55\n","Epoch 2: Best 90.55\n","100% 500/500 [03:00<00:00,  2.78it/s]\n","\n","Loss: tensor(0.2444, device='cuda:0', grad_fn=<NllLossBackward>)\n","\n","Epoch 3: Train 91.11\n","Epoch 3: Valid 93.00\n","Epoch 3: Best 93.00\n","100% 500/500 [02:58<00:00,  2.80it/s]\n","\n","Loss: tensor(0.1051, device='cuda:0', grad_fn=<NllLossBackward>)\n","\n","Epoch 4: Train 92.62\n","Epoch 4: Valid 94.05\n","Epoch 4: Best 94.05\n","100% 500/500 [02:57<00:00,  2.82it/s]\n","\n","Loss: tensor(0.1425, device='cuda:0', grad_fn=<NllLossBackward>)\n","\n","Epoch 5: Train 93.88\n","Epoch 5: Valid 94.40\n","Epoch 5: Best 94.40\n","100% 500/500 [02:57<00:00,  2.81it/s]\n","\n","Loss: tensor(0.0428, device='cuda:0', grad_fn=<NllLossBackward>)\n","\n","Epoch 6: Train 94.57\n","Epoch 6: Valid 94.30\n","Epoch 6: Best 94.40\n","100% 500/500 [02:58<00:00,  2.81it/s]\n","\n","Loss: tensor(0.1272, device='cuda:0', grad_fn=<NllLossBackward>)\n","\n","Epoch 7: Train 94.92\n","Epoch 7: Valid 93.85\n","Epoch 7: Best 94.40\n","100% 500/500 [02:54<00:00,  2.87it/s]\n","\n","Loss: tensor(0.2958, device='cuda:0', grad_fn=<NllLossBackward>)\n","\n","Epoch 8: Train 95.51\n","Epoch 8: Valid 93.25\n","Epoch 8: Best 94.40\n","Load 16000 data from split(s) train.\n","Start to load Faster-RCNN detected objects from data/nlvr2_imgfeat/train_obj36.tsv\n","Loaded 512 images in file data/nlvr2_imgfeat/train_obj36.tsv in 2 seconds.\n","Use 512 data in torch dataset\n","\n","Load 2000 data from split(s) valid.\n","Start to load Faster-RCNN detected objects from data/nlvr2_imgfeat/valid_obj36.tsv\n","Loaded 512 images in file data/nlvr2_imgfeat/valid_obj36.tsv in 2 seconds.\n","Use 512 data in torch dataset\n","\n","LXRT encoder with 9 l_layers, 5 x_layers, and 5 r_layers.\n","Total Iters: 0\n","Load model from snap/nlvr2/nlvr2_lxr955/BEST\n","Load 2000 data from split(s) test.\n","Start to load Faster-RCNN detected objects from data/nlvr2_imgfeat/test_obj36.tsv\n","Loaded 2000 images in file data/nlvr2_imgfeat/test_obj36.tsv in 9 seconds.\n","Use 2000 data in torch dataset\n","\n","0.9335\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0o4nVK0KdyX1","colab_type":"text"},"source":["Load finetuned model"]},{"cell_type":"code","metadata":{"id":"hGQ0K-n7ldrF","colab_type":"code","colab":{}},"source":["!cd /content/lxmert/snap/ && mkdir nlvr2 && cd nlvr2 && mkdir nlvr2_lxr955 \n","!cp \"/content/drive/My Drive/best_models/BEST_single.pth\" \"/content/lxmert/snap/nlvr2/nlvr2_lxr955/BEST.pth\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LNbUkjj_eQpZ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":410},"executionInfo":{"status":"ok","timestamp":1596897605040,"user_tz":-120,"elapsed":75530,"user":{"displayName":"yulya sot","photoUrl":"https://lh6.googleusercontent.com/-ly9qpHtKZbM/AAAAAAAAAAI/AAAAAAAAAHs/3g5vNREGJNw/s64/photo.jpg","userId":"08271027061435931011"}},"outputId":"e0ef70f6-43ec-46a5-8e67-b1e1d25645d0"},"source":["!cd lxmert/ && bash run/nlvr2_test.bash 0 nlvr2_lxr955_results --load snap/nlvr2/nlvr2_lxr955/BEST --test test --batchSize 250"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Load 16000 data from split(s) train.\n","Start to load Faster-RCNN detected objects from data/nlvr2_imgfeat/train_obj36.tsv\n","Loaded 512 images in file data/nlvr2_imgfeat/train_obj36.tsv in 2 seconds.\n","Use 512 data in torch dataset\n","\n","Load 2000 data from split(s) valid.\n","Start to load Faster-RCNN detected objects from data/nlvr2_imgfeat/valid_obj36.tsv\n","Loaded 512 images in file data/nlvr2_imgfeat/valid_obj36.tsv in 2 seconds.\n","Use 512 data in torch dataset\n","\n","100% 231508/231508 [00:00<00:00, 885846.14B/s]\n","100% 407873900/407873900 [00:11<00:00, 35287758.94B/s]\n","LXRT encoder with 9 l_layers, 5 x_layers, and 5 r_layers.\n","Total Iters: 20\n","Load model from snap/nlvr2/nlvr2_lxr955/BEST\n","Load 2000 data from split(s) test.\n","Start to load Faster-RCNN detected objects from data/nlvr2_imgfeat/test_obj36.tsv\n","Loaded 2000 images in file data/nlvr2_imgfeat/test_obj36.tsv in 21 seconds.\n","Use 2000 data in torch dataset\n","\n","0.935\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WbZ0ayNCh6pq","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}